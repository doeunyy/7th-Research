### Paper Details
- **Title**: Flamingo: A Visual Language Model for Few-Shot Learning
- **Authors**: Jean-Baptiste Alayrac*, Jeff Donahue*, Pauline Luc*, Antoine Miech*, Iain Barr‚Ä†, Yana Hasson‚Ä†, Karel Lenc‚Ä†, Arthur Mensch‚Ä†, Katie Millican‚Ä†, Malcolm Reynolds‚Ä†, Roman Ring‚Ä†, Eliza Rutherford‚Ä†, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, Karen Simonyan* (* indicates equal contributions)
- **Conference**: 36th Conference on Neural Information Processing Systems (NeurIPS)
- **Year of Publication**: 2022
- **Link**: [https://arxiv.org/abs/2204.14198](https://arxiv.org/abs/2204.14198)
- **Key Focus**: Flamingo is a Visual Language Model (VLM) designed for few-shot learning, enabling rapid adaptation to multimodal tasks with minimal task-specific data. By integrating pre-trained vision and language models through innovations like GATED XATTN-DENSE layers and the Perceiver Resampler, Flamingo processes interleaved sequences of text, images, and videos to generate context-aware outputs. It achieves state-of-the-art performance on diverse benchmarks without fine-tuning, demonstrating strong generalization by leveraging large-scale web-sourced multimodal datasets. This model sets a new standard for efficient and versatile multimodal learning.


### Link
üìù [[ÎÖºÎ¨∏ Î¶¨Î∑∞] Flamingo: A Visual Language Model for Few-Shot Learning](https://dony-archive.tistory.com/43)
